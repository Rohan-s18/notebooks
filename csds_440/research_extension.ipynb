{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Extension\n",
    "### Author: Rohan Singh, Sophia Hall and Mohammed Otefi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Hypothesis\n",
    "We hypothesize that a two-step classification approach combining a logistic regression classifier and a decision tree using the ID3 algorithm can improve prediction accuracy for a given problem. \n",
    "\n",
    "First, we propose to use logistic regression to predict a binary label based on a set of input features. The logistic regression classifier will provide probability estimates for the positive class, which can serve as valuable additional information. \n",
    "\n",
    "Subsequently, we intend to incorporate these probability estimates, along with other relevant features, into a decision tree model constructed using the ID3 algorithm. The decision tree will leverage both the original features and the logistic regression output to make more informed and accurate predictions. We believe that this two-step approach can capture complex relationships between the input features and the target label, leading to improved predictive performance compared to using either logistic regression or the ID3 algorithm independently. In addition to providing a more robust network for classification, we can also avoid the possibiilty of overfitting data points as we are using multiple classifiers rather than just a single classifier.\n",
    "\n",
    "Such a network of a Logistic Regression Classifier feeding into a decision tree can also benefit from creating multi-layer decision trees, where we have multiple different trained-classifiers, which may be trainde on different subsets of features, feeding information into a decision tree in a way similar to how most deep learning neural networks work. In a way, adding neurons to a decision tree to make it a **Smart Decision Tree**.\n",
    "\n",
    "The hypothesis will be tested and validated through empirical experiments and evaluation metrics to assess the effectiveness of the proposed approach in enhancing predictive accuracy and model interpretability. The newly written code for testing and experimental purposes is purely a proof of concept code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier\n",
    "Logistic regression is a binary classification algorithm that uses the logistic (sigmoid) function to model the probability of an input belonging to a particular class. The logistic regression model works by finding the relationship between the input features and the binary outcome, which can be thought of as predicting a probability.\n",
    "\n",
    "Here's how the logistic regression model works and how it is trained:\n",
    "\n",
    "1. **Model Hypothesis**: The logistic regression model uses the following hypothesis function:\n",
    "\n",
    "   $$h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\n",
    "\n",
    "   Where:\n",
    "   - $h_{\\theta}(x)$ is the predicted probability that input $x$ belongs to the positive class.\n",
    "   - $\\theta$ is a vector of model parameters (weights), and $\\theta^Tx$ is the dot product of the parameters and the input features.\n",
    "\n",
    "   The sigmoid function ($\\frac{1}{1 + e^{-z}}$) maps any real-valued number $z$ to the range (0, 1), making it suitable for representing probabilities.\n",
    "\n",
    "2. **Training**: The goal of training a logistic regression model is to find the best values of the parameter vector $\\theta$ that minimize a cost function, typically the logistic loss (also called cross-entropy loss) for binary classification:\n",
    "\n",
    "   $$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})\\log(1 - h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "   Where:\n",
    "   - $J(\\theta)$ is the cost function that measures the error of the model.\n",
    "   - $m$ is the number of training examples.\n",
    "   - $x^{(i)}$ is the feature vector of the $i$-th training example.\n",
    "   - $y^{(i)}$ is the true label (0 for negative class, 1 for positive class) of the $i$-th example.\n",
    "   - $h_{\\theta}(x^{(i)})$ is the predicted probability of $x^{(i)}$ belonging to the positive class.\n",
    "\n",
    "3. **Optimization**: To minimize the cost function $J(\\theta)$, you can use optimization techniques like gradient descent. The gradient descent algorithm updates the parameters $\\theta$ iteratively as follows:\n",
    "\n",
    "   $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "   Where:\n",
    "   - $\\alpha$ is the learning rate, which controls the step size in the parameter updates.\n",
    "   - $\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$ is the partial derivative of the cost function with respect to $\\theta_j$.\n",
    "\n",
    "4. **Repeat**: Steps 2 and 3 are repeated until convergence, i.e., until the cost function stops decreasing significantly, or a predetermined number of iterations is reached.\n",
    "\n",
    "5. **Prediction**: Once the model is trained and the optimal $\\theta$ is found, you can use the hypothesis function to make predictions on new data. If $h_{\\theta}(x) \\geq 0.5$, you classify the input as belonging to the positive class (1), otherwise, you classify it as belonging to the negative class (0).\n",
    "\n",
    "In summary, logistic regression uses the sigmoid function to model the probability of an input belonging to a class. It is trained by minimizing the logistic loss function using techniques like gradient descent, and then it makes binary classification predictions based on the learned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Proof of Concept Decision Tree\n",
    "This cell contains code for a simple decision tree trained over **Gain Ratio** that we are using as a becnhmark for our new algorithm. We are not using the original decision tree that we implemented under the source folder, as we wish to use this solely for experimental purposes and want to use only a subset of the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.feature = feature          # Splitting feature (column index)\n",
    "        self.threshold = threshold      # Threshold value for continuous attributes\n",
    "        self.value = value              # Predicted class (for leaf nodes)\n",
    "        self.left = left                # Left subtree (for continuous attributes)\n",
    "        self.right = right              # Right subtree (for continuous attributes)\n",
    "        self.children = {}              # Child nodes (for categorical attributes)\n",
    "\n",
    "class DecisionTreeID3:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self.max_label = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.max_label = np.argmax(np.bincount(y))\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        default_class = unique_classes[np.argmax(counts)]\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (depth == self.max_depth) or (len(unique_classes) == 1) or (n_samples < 2):\n",
    "            return Node(value=default_class)\n",
    "\n",
    "        best_gain_ratio = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        is_continuous = False\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "\n",
    "            if len(unique_values) > 5:  # Threshold for considering a feature as continuous\n",
    "                is_continuous = True\n",
    "                thresholds = self._find_continuous_split_points(X[:, feature])\n",
    "                for threshold in thresholds:\n",
    "                    gain_ratio = self._compute_gain_ratio(X, y, feature, threshold)\n",
    "                    if gain_ratio > best_gain_ratio:\n",
    "                        best_gain_ratio = gain_ratio\n",
    "                        best_feature = feature\n",
    "                        best_threshold = threshold\n",
    "            else:\n",
    "                gain_ratio = self._compute_gain_ratio(X, y, feature)\n",
    "                if gain_ratio > best_gain_ratio:\n",
    "                    best_gain_ratio = gain_ratio\n",
    "                    best_feature = feature\n",
    "\n",
    "        if best_gain_ratio == 0:\n",
    "            return Node(value=default_class)\n",
    "\n",
    "        if is_continuous:\n",
    "            left_indices = X[:, best_feature] <= best_threshold\n",
    "            right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "            left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "            right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "            return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "        else:\n",
    "            node = Node(feature=best_feature)\n",
    "            unique_values = np.unique(X[:, best_feature])\n",
    "            for value in unique_values:\n",
    "                value_indices = X[:, best_feature] == value\n",
    "                node.children[value] = self._build_tree(X[value_indices], y[value_indices], depth + 1)\n",
    "            return node\n",
    "\n",
    "    def _compute_entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _compute_information_gain(self, y, split_indices):\n",
    "        entropy_parent = self._compute_entropy(y)\n",
    "        weighted_entropy_children = 0\n",
    "\n",
    "        for indices in split_indices:\n",
    "            weighted_entropy_children += (len(indices) / len(y)) * self._compute_entropy(y[indices])\n",
    "\n",
    "        information_gain = entropy_parent - weighted_entropy_children\n",
    "        return information_gain\n",
    "\n",
    "    def _compute_split_info(self, y, split_indices):\n",
    "        split_info = 0\n",
    "\n",
    "        for indices in split_indices:\n",
    "            if len(indices) > 0:\n",
    "                p_i = len(indices) / len(y)\n",
    "                split_info -= p_i * np.log2(p_i)\n",
    "\n",
    "        return split_info\n",
    "\n",
    "    def _compute_gain_ratio(self, X, y, feature, threshold=None):\n",
    "        if threshold is None:\n",
    "            values = X[:, feature]\n",
    "            split_indices = [np.where(values == v)[0] for v in np.unique(values)]\n",
    "        else:\n",
    "            values = X[:, feature]\n",
    "            split_indices = [np.where(values <= threshold)[0], np.where(values > threshold)[0]]\n",
    "\n",
    "        information_gain = self._compute_information_gain(y, split_indices)\n",
    "        split_info = self._compute_split_info(y, split_indices)\n",
    "\n",
    "        if split_info == 0:\n",
    "            return 0  # Avoid division by zero\n",
    "\n",
    "        gain_ratio = information_gain / split_info\n",
    "        return gain_ratio\n",
    "\n",
    "    def _find_continuous_split_points(self, values):\n",
    "        sorted_values = np.sort(np.unique(values))\n",
    "        split_points = [(sorted_values[i] + sorted_values[i + 1]) / 2 for i in range(len(sorted_values) - 1)]\n",
    "        return split_points\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if node.feature is not None:\n",
    "            if node.feature in x:\n",
    "                if node.feature in node.children:\n",
    "                    return self._predict_tree(x, node.children[node.feature])\n",
    "                else:\n",
    "                    return self.max_label\n",
    "            else:\n",
    "                return self.max_label\n",
    "\n",
    "    def _predict_continuous(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if node.feature is not None:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return self._predict_continuous(x, node.left)\n",
    "            else:\n",
    "                return self._predict_continuous(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with continuous and categorical attributes\n",
    "data = pd.DataFrame({\n",
    "        'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy'],\n",
    "        'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75],\n",
    "        'Humidity': [85, 90, 86, 96, 80, 70, 65, 95, 70, 80],\n",
    "        'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Strong'],\n",
    "        'PlayTennis': [0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "X = data.drop('PlayTennis', axis=1).values\n",
    "y = data['PlayTennis'].values\n",
    "\n",
    "dt = DecisionTreeID3(max_depth=3)\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Logistic Regression Classifier\n",
    "Here we have provided the code for a logistic regression classifier that we mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.1, num_iterations=100000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / m) * np.dot(X.T, (predictions - y))\n",
    "            db = (1 / m) * np.sum(predictions - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self.sigmoid(linear_model)\n",
    "        return [1 if p >= 0.95 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data[['Temperature','Humidity','PlayTennis']]\n",
    "\n",
    "X = df_train.drop('PlayTennis', axis=1).values\n",
    "y = df_train['PlayTennis'].values\n",
    "\n",
    "regressor = LogisticRegression()\n",
    "regressor.fit(X,y)\n",
    "\n",
    "# Adding a new feature which is the predicted values from the Logistic Regression Classifier\n",
    "logistic_predictions = regressor.predict(X)\n",
    "\n",
    "data[\"regressor_predictions\"] = logistic_predictions\n",
    "\n",
    "\n",
    "X = data.drop('PlayTennis', axis=1).values\n",
    "y = data['PlayTennis'].values\n",
    "\n",
    "dt = DecisionTreeID3(max_depth=3)\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "To run the experiments to prove our hypothesis correct, we have chosen to test them on the iris dataset, which is loaded using scikit-learn's load_iris function. After training both decision trees on that dataset we then compare their accuracy by using scikit-learn's accuracy score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Used\n",
    "The metrics used for the purposes of comparing the results of both implementations, we are using scikit-learn's accuracy score fucntion as the baseline comparison method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df['target'].replace(2, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the decision tree: 66.67%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_dt = iris_df.drop('target', axis=1).values\n",
    "y_dt = iris_df['target'].values\n",
    "\n",
    "dt = DecisionTreeID3(max_depth=3)\n",
    "dt.fit(X_dt, y_dt)\n",
    "\n",
    "y_pred = dt.predict(X_dt)\n",
    "\n",
    "accuracy = accuracy_score(y_dt, y_pred)\n",
    "print(f\"Accuracy of the decision tree: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on modified Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the decision tree: 66.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris_df_2 = iris_df\n",
    "\n",
    "X_mdt = iris_df_2.drop('target', axis=1).values\n",
    "y_mdt = iris_df_2['target'].values\n",
    "\n",
    "regressor = LogisticRegression()\n",
    "regressor.fit(X_mdt,y_mdt)\n",
    "\n",
    "# Adding a new feature which is the predicted values from the Logistic Regression Classifier\n",
    "logistic_predictions = regressor.predict(X_mdt)\n",
    "\n",
    "iris_df_2[\"regressor_predictions\"] = logistic_predictions\n",
    "\n",
    "\n",
    "X_train = iris_df_2.drop('target', axis=1).values\n",
    "y_train = iris_df_2['target'].values\n",
    "\n",
    "dt_2 = DecisionTreeID3(max_depth=3)\n",
    "dt_2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_mdt = dt_2.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_mdt)\n",
    "print(f\"Accuracy of the decision tree: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Through running experiments on this proof on concept decision tree and its modified version, we have shown that adding a logistic regression classifier with sigmoid non-linearity as a feature can lead to an increase in performance to our original ID3 decision tree. As mentioned in the hypothesis we can further improve this by adding different non-linearities such as RELU or step functions as well to create deep-learning networks. Additionally, such classifiers are useful to create good mappings between a contuinous vector space to a binary space, and with the help of more efficient optimizers can lead to a significant speedup of training decision trees by simply training them on those binary attributes rather than training them on a large spectrum of continous variables which takes up a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax Bug\n",
    "int x = 7-9;\n",
    "\n",
    "# Logical Bug\n",
    "while True:\n",
    "    pass\n",
    "\n",
    "# Functional Bug\n",
    "y = 9-7\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
